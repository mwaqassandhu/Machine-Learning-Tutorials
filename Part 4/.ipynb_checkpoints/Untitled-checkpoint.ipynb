{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e944dde",
   "metadata": {},
   "source": [
    "# Tutorial on Natural Language Processing (NLP): Text Analysis and Sentiment Classification\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction to Natural Language Processing (NLP)\n",
    "2. Preprocessing Text Data\n",
    "3. Tokenization\n",
    "4. Stopword Removal\n",
    "5. Lemmatization or Stemming\n",
    "6. Feature Extraction\n",
    "7. Sentiment Analysis\n",
    "8. Building a Sentiment Classifier\n",
    "9. Conclusion and Further Steps\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing (NLP) is a branch of artificial intelligence that deals with the interaction between computers and human language. It enables machines to understand, interpret, and generate human language in a way that is valuable and useful.\n",
    "\n",
    "## 2. Preprocessing Text Data\n",
    "\n",
    "Before applying NLP techniques, it's essential to preprocess the text data. This involves cleaning and preparing the data for analysis.\n",
    "\n",
    "### 2.1. Text Cleaning\n",
    "- Remove any special characters, punctuation, and numbers.\n",
    "- Convert the text to lowercase for uniformity.\n",
    "\n",
    "### 2.2. Handling Missing Data\n",
    "- Check for and handle any missing or null values.\n",
    "\n",
    "## 3. Tokenization\n",
    "\n",
    "Tokenization involves splitting text into individual words or tokens. It's a crucial step for any NLP task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda9d4d",
   "metadata": {},
   "source": [
    "### Example in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de34a38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\drwaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Natural Language Processing is fun!\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb24e8f",
   "metadata": {},
   "source": [
    "## 4. Stopword Removal\n",
    "\n",
    "Stopwords are common words (e.g., \"the\", \"and\", \"is\") that do not carry much information. Removing them can help reduce noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0e7324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'fun', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\drwaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f0faf",
   "metadata": {},
   "source": [
    "## 5. Lemmatization or Stemming\n",
    "\n",
    "Lemmatization and stemming reduce words to their base or root form, which helps in normalizing the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84395af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drwaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\drwaq\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "### Example in Python (using NLTK for Lemmatization):\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f3b28",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction\n",
    "\n",
    "To analyze text, it needs to be represented numerically. Two common techniques are Bag-of-Words and TF-IDF (Term Frequency-Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d916936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing fun !']\n",
      "  (0, 0)\t0.5\n",
      "  (0, 3)\t0.5\n",
      "  (0, 1)\t0.5\n",
      "  (0, 2)\t0.5\n"
     ]
    }
   ],
   "source": [
    "### Example in Python (using TF-IDF with scikit-learn):\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [' '.join(lemmatized_tokens)]\n",
    "print(corpus)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c06dc7",
   "metadata": {},
   "source": [
    "## 7. Sentiment Analysis\n",
    "\n",
    "Sentiment analysis aims to determine the sentiment or emotion expressed in a piece of text. It can be positive, negative, or neutral.\n",
    "\n",
    "## 8. Building a Sentiment Classifier\n",
    "\n",
    "### Example in Python (using scikit-learn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45712880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have a dataset with labeled sentiments (positive/negative)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict sentiment\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8f1da",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Further Steps\n",
    "\n",
    "Congratulations! You've completed a basic tutorial on NLP, covering text analysis and sentiment classification. To enhance your skills, you can explore more advanced techniques, work with larger datasets, and experiment with different machine learning models. Additionally, consider diving into other NLP tasks like named entity recognition, text summarization, and machine translation. Keep learning and experimenting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693b1df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
