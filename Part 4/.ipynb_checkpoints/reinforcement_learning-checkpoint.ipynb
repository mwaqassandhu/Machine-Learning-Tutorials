{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7fa35c",
   "metadata": {},
   "source": [
    "# Tutorial: Reinforcement Learning with Python\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties for its actions, and the goal is to learn a policy that maximizes the cumulative reward over time. In this tutorial, we'll implement a basic RL algorithm called Q-Learning using Python.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Python (3.6 or higher)\n",
    "2. Basic understanding of Python programming\n",
    "3. Familiarity with numpy and matplotlib is helpful but not required.\n",
    "\n",
    "## Step 1: Setting up the Environment\n",
    "\n",
    "For this tutorial, we'll use a simple grid-world environment. Each cell in the grid represents a state, and the agent can take actions to move around the grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db350df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = (6, 6)\n",
    "        self.start_state = (0, 0)\n",
    "        self.goal_state = (5, 5)\n",
    "        self.current_state = self.start_state\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "    def reset(self, start_state = (0,0)):\n",
    "        self.current_state = start_state\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 'up' and self.current_state[0] > 0:\n",
    "            self.current_state = (self.current_state[0] - 1, self.current_state[1])\n",
    "        elif action == 'down' and self.current_state[0] < self.grid_size[0] - 1:\n",
    "            self.current_state = (self.current_state[0] + 1, self.current_state[1])\n",
    "        elif action == 'left' and self.current_state[1] > 0:\n",
    "            self.current_state = (self.current_state[0], self.current_state[1] - 1)\n",
    "        elif action == 'right' and self.current_state[1] < self.grid_size[1] - 1:\n",
    "            self.current_state = (self.current_state[0], self.current_state[1] + 1)\n",
    "\n",
    "        if self.current_state == self.goal_state:\n",
    "            return self.current_state, 1.0, True\n",
    "        else:\n",
    "            return self.current_state, -0.1, False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac9c08",
   "metadata": {},
   "source": [
    "## Step 2: Q-Learning Algorithm\n",
    "\n",
    "Next, let's implement the Q-Learning algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f17666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, exploration_prob=0.1):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_prob = exploration_prob\n",
    "        self.q_table = np.zeros((len(env.actions), env.grid_size[0], env.grid_size[1]))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.uniform(0, 1) < self.exploration_prob:\n",
    "            return random.choice(self.env.actions)\n",
    "        else:\n",
    "            action_values = self.q_table[:, state[0], state[1]]\n",
    "            return self.env.actions[np.argmax(action_values)]\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        action_index = self.env.actions.index(action)\n",
    "        current_value = self.q_table[action_index, state[0], state[1]]\n",
    "        next_max_value = np.max(self.q_table[:, next_state[0], next_state[1]])\n",
    "        updated_value = (1 - self.learning_rate) * current_value + self.learning_rate * (reward + self.discount_factor * next_max_value)\n",
    "        self.q_table[action_index, state[0], state[1]] = updated_value\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                self.update_q_table(state, action, reward, next_state)\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b60b3",
   "metadata": {},
   "source": [
    "## Step 3: Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f836f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and agent\n",
    "env = GridWorld()\n",
    "agent = QLearningAgent(env)\n",
    "\n",
    "# Train the agent\n",
    "num_episodes = 1000\n",
    "agent.train(num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30353757",
   "metadata": {},
   "source": [
    "## Step 4: Evaluating the Agent\n",
    "\n",
    "You can now evaluate the agent's performance using the learned Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb121f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e18c2a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: 0.0019999999999999606\n"
     ]
    }
   ],
   "source": [
    "def evaluate_agent(agent, num_episodes):\n",
    "    total_rewards = 0\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_rewards += reward\n",
    "            state = next_state\n",
    "    return total_rewards / num_episodes\n",
    "\n",
    "# Evaluate the agent\n",
    "num_eval_episodes = 100\n",
    "avg_reward = evaluate_agent(agent, num_eval_episodes)\n",
    "print(f'Average Reward: {avg_reward}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dacc65cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . G \n",
      "\n",
      ". A . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . G \n",
      "\n",
      ". . A . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . G \n",
      "\n",
      ". . . . . . \n",
      ". . A . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . G \n",
      "\n",
      ". . . . . . \n",
      ". . . A . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . G \n",
      "\n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . A . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . G \n",
      "\n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . A . . \n",
      ". . . . . . \n",
      ". . . . . G \n",
      "\n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . A . \n",
      ". . . . . . \n",
      ". . . . . G \n",
      "\n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . A . \n",
      ". . . . . G \n",
      "\n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . . . \n",
      ". . . . A G \n"
     ]
    }
   ],
   "source": [
    "def print_grid_world(env):\n",
    "    print(\"\")\n",
    "    for i in range(env.grid_size[0]):\n",
    "        for j in range(env.grid_size[1]):\n",
    "            if (i, j) == env.current_state:\n",
    "                print(\"A\", end=\" \")  # Agent's position\n",
    "            elif (i, j) == env.goal_state:\n",
    "                print(\"G\", end=\" \")  # Goal position\n",
    "            else:\n",
    "                print(\".\", end=\" \")  # Empty cell\n",
    "        print()\n",
    "\n",
    "        \n",
    "def play_trained_agent(agent, env):\n",
    "    start_state = (0,0)\n",
    "    state = env.reset(start_state)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        print_grid_world(env)  # Print the grid world\n",
    "        action = agent.select_action(state)\n",
    "        next_state, _, done = env.step(action)\n",
    "        state = next_state\n",
    "        \n",
    "        \n",
    "# Usage:\n",
    "play_trained_agent(agent, env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadb7db",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've now implemented a basic Q-Learning agent in Python. This is just a starting point, and there are many extensions and improvements you can make to this algorithm. Further exploration could involve more complex environments, different RL algorithms, or applying RL to real-world problems. Happy coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83637e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
