{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7f7fe8",
   "metadata": {},
   "source": [
    "# Tutorial on Natural Language Processing (NLP): Text Analysis and Sentiment Classification\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction to Natural Language Processing (NLP)\n",
    "2. Preprocessing Text Data\n",
    "3. Tokenization\n",
    "4. Stopword Removal\n",
    "5. Lemmatization or Stemming\n",
    "6. Feature Extraction\n",
    "7. Sentiment Analysis\n",
    "8. Building a Sentiment Classifier\n",
    "9. Conclusion and Further Steps\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing (NLP) is a branch of artificial intelligence that deals with the interaction between computers and human language. It enables machines to understand, interpret, and generate human language in a way that is valuable and useful.\n",
    "\n",
    "## 2. Preprocessing Text Data\n",
    "\n",
    "Before applying NLP techniques, it's essential to preprocess the text data. This involves cleaning and preparing the data for analysis.\n",
    "\n",
    "### 2.1. Text Cleaning\n",
    "- Remove any special characters, punctuation, and numbers.\n",
    "- Convert the text to lowercase for uniformity.\n",
    "\n",
    "### 2.2. Handling Missing Data\n",
    "- Check for and handle any missing or null values.\n",
    "\n",
    "## 3. Tokenization\n",
    "\n",
    "Tokenization involves splitting text into individual words or tokens. It's a crucial step for any NLP task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066b8fc",
   "metadata": {},
   "source": [
    "### Example in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf90ac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\drwaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Natural Language Processing is fun!\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2615c4",
   "metadata": {},
   "source": [
    "## 4. Stopword Removal\n",
    "\n",
    "Stopwords are common words (e.g., \"the\", \"and\", \"is\") that do not carry much information. Removing them can help reduce noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa32fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'fun', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\drwaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefe7a31",
   "metadata": {},
   "source": [
    "## 5. Lemmatization or Stemming\n",
    "\n",
    "Lemmatization and stemming reduce words to their base or root form, which helps in normalizing the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69020107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drwaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\drwaq\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "### Example in Python (using NLTK for Lemmatization):\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459cb0c9",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction\n",
    "\n",
    "To analyze text, it needs to be represented numerically. Two common techniques are Bag-of-Words and TF-IDF (Term Frequency-Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda5ca51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing fun !']\n",
      "  (0, 0)\t0.5\n",
      "  (0, 3)\t0.5\n",
      "  (0, 1)\t0.5\n",
      "  (0, 2)\t0.5\n"
     ]
    }
   ],
   "source": [
    "### Example in Python (using TF-IDF with scikit-learn):\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [' '.join(lemmatized_tokens)]\n",
    "print(corpus)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07352ad",
   "metadata": {},
   "source": [
    "## 7. Sentiment Analysis\n",
    "\n",
    "Sentiment analysis aims to determine the sentiment or emotion expressed in a piece of text. It can be positive, negative, or neutral.\n",
    "\n",
    "## 8. Building a Sentiment Classifier\n",
    "\n",
    "### Example in Python (using scikit-learn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform sentiment analysis on IMDb movie reviews, you'll first need to obtain the dataset. The IMDb dataset is a popular choice for sentiment analysis tasks, as it contains a large number of movie reviews labeled with their corresponding sentiment (positive or negative).\n",
    "\n",
    "Here's a step-by-step guide:\n",
    "\n",
    "### Step 1: Download the IMDb Dataset\n",
    "\n",
    "You can download the dataset from the [IMDb website](https://ai.stanford.edu/~amaas/data/sentiment/). It consists of two compressed files: `aclImdb_v1.tar.gz` and `aclImdb_v1.tar`. You can download and extract them to a suitable location on your computer.\n",
    "\n",
    "### Step 2: Preprocess the Data\n",
    "\n",
    "Once you have the dataset, you'll need to preprocess it for analysis. This involves reading the reviews and their corresponding labels (positive or negative).\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "def load_data(folder):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        label_folder = os.path.join(folder, label)\n",
    "        for filename in os.listdir(label_folder):\n",
    "            with open(os.path.join(label_folder, filename), 'r', encoding='utf-8') as file:\n",
    "                texts.append(file.read())\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = load_data('aclImdb/train')\n",
    "test_texts, test_labels = load_data('aclImdb/test')\n",
    "```\n",
    "\n",
    "### Step 3: Preprocess Text Data\n",
    "\n",
    "Apply the text preprocessing steps mentioned in the earlier tutorial:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Text cleaning, tokenization, stopwords removal, lemmatization\n",
    "# ...\n",
    "\n",
    "# Feature extraction (TF-IDF)\n",
    "# ...\n",
    "```\n",
    "\n",
    "### Step 4: Build and Train the Model\n",
    "\n",
    "You can use various machine learning models like Logistic Regression, Support Vector Machines, or even deep learning models like LSTM or BERT for sentiment analysis. For simplicity, I'll use Logistic Regression as in the previous tutorial.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have already preprocessed the text data and obtained features (X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict sentiment\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "```\n",
    "\n",
    "### Step 5: Evaluate the Model\n",
    "\n",
    "After training, you can evaluate the model's performance using metrics like accuracy, precision, recall, etc.\n",
    "\n",
    "### Step 6: Further Steps\n",
    "\n",
    "You can further improve the model by trying different preprocessing techniques, experimenting with different models, or using more advanced techniques like deep learning with embeddings or transformer models.\n",
    "\n",
    "Remember to fine-tune the hyperparameters and consider techniques like cross-validation for a more robust evaluation.\n",
    "\n",
    "Keep in mind that this is a basic tutorial. In practice, NLP tasks can get much more complex, and there are many advanced techniques to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have a dataset with labeled sentiments (positive/negative)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict sentiment\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a448ea",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Further Steps\n",
    "\n",
    "Congratulations! You've completed a basic tutorial on NLP, covering text analysis and sentiment classification. To enhance your skills, you can explore more advanced techniques, work with larger datasets, and experiment with different machine learning models. Additionally, consider diving into other NLP tasks like named entity recognition, text summarization, and machine translation. Keep learning and experimenting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252c036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
