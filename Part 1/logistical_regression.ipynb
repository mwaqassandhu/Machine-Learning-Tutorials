{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbe71e3",
   "metadata": {},
   "source": [
    "# Tutorial: Logistic Regression for Binary Classification and Probability Estimation\n",
    "\n",
    "Logistic Regression is a widely used statistical technique for binary classification and probability estimation tasks. It's especially useful when you need to predict whether an instance belongs to one of two classes or when you want to estimate the probability of an instance belonging to a particular class. In this tutorial, we'll cover the following topics:\n",
    "\n",
    "1. **Introduction to Logistic Regression**\n",
    "2. **Logistic Regression Equation**\n",
    "3. **Sigmoid Function**\n",
    "4. **Training Logistic Regression**\n",
    "5. **Making Predictions**\n",
    "6. **Evaluating the Model**\n",
    "7. **Probability Estimation**\n",
    "8. **Code Example in Python**\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## 1. Introduction to Logistic Regression\n",
    "\n",
    "Logistic Regression is a type of regression analysis used for predicting the probability of a binary outcome. It's named \"logistic\" because it's based on the logistic function (or sigmoid function), which maps any real-valued number to a value between 0 and 1. Logistic Regression is widely used in various fields, including medicine, finance, and machine learning.\n",
    "\n",
    "## 2. Logistic Regression Equation\n",
    "\n",
    "The logistic regression equation is:\n",
    "\n",
    "$$ P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}} $$\n",
    "\n",
    "Where:\n",
    "- $ P(Y=1|X) $ is the probability that the dependent variable $ Y $ equals 1 given the independent variable $ X $.\n",
    "- $ X $ is the input features or predictors.\n",
    "- $ \\beta_0 $ is the intercept.\n",
    "- $ \\beta_1 $ is the coefficient associated with the independent variable $ X $.\n",
    "- $ e $ is the base of the natural logarithm (approximately 2.71828).\n",
    "\n",
    "## 3. Sigmoid Function\n",
    "\n",
    "The sigmoid function $ \\sigma(z) $, also known as the logistic function, is the core of logistic regression. It maps any real-valued number $ z $ to a value between 0 and 1. The formula for the sigmoid function is:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "The sigmoid function has an S-shaped curve, which makes it suitable for modeling probabilities in binary classification problems.\n",
    "\n",
    "## 4. Training Logistic Regression\n",
    "\n",
    "The goal of training logistic regression is to find the best values for the coefficients $ \\beta_0 $ and $ \\beta_1 $ that minimize the error in predicting the target variable. This is typically done using optimization techniques like gradient descent.\n",
    "\n",
    "## 5. Making Predictions\n",
    "\n",
    "Once the model is trained, you can make predictions on new data points. You plug the values of the independent variables into the logistic regression equation, and the output will be a probability score between 0 and 1. By setting a threshold (e.g., 0.5), you can classify instances into one of the two classes.\n",
    "\n",
    "## 6. Evaluating the Model\n",
    "\n",
    "Let's dive into more detail about the metrics used to evaluate the performance of a logistic regression model in binary classification.\n",
    "\n",
    "**1. Accuracy:** Accuracy is a common metric used to evaluate classification models, including logistic regression. It measures the percentage of correctly predicted instances out of all instances. The formula for accuracy is:\n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} $$\n",
    "\n",
    "While accuracy is a useful metric, it may not provide a complete picture of a model's performance, especially when dealing with imbalanced datasets.\n",
    "\n",
    "**2. Precision:** Precision is a metric that focuses on the accuracy of positive predictions. It answers the question: \"Of all the instances predicted as positive, how many were actually positive?\" Precision is important when false positives are costly. The formula for precision is:\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}} $$\n",
    "\n",
    "**3. Recall (Sensitivity or True Positive Rate):** Recall measures the ability of the model to correctly identify all relevant instances. It answers the question: \"Of all the actual positive instances, how many did the model correctly predict as positive?\" Recall is important when false negatives are costly. The formula for recall is:\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}} $$\n",
    "\n",
    "**4. F1-Score:** The F1-score is the harmonic mean of precision and recall. It balances the trade-off between precision and recall and is especially useful when the dataset is imbalanced. The formula for the F1-score is:\n",
    "\n",
    "$$ \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
    "\n",
    "The F1-score is a useful metric when you want to find a balance between precision and recall.\n",
    "\n",
    "**5. ROC Curve (Receiver Operating Characteristic Curve):** The ROC curve is a graphical representation of a model's performance across different thresholds for classifying instances. It plots the true positive rate (recall) against the false positive rate at various threshold values. A model with a higher area under the ROC curve (AUC-ROC) is generally considered better at distinguishing between classes. An AUC-ROC of 0.5 indicates a model that performs no better than random guessing, while an AUC-ROC of 1.0 represents a perfect classifier.\n",
    "\n",
    "These metrics provide a comprehensive view of how well a logistic regression model is performing in binary classification tasks:\n",
    "\n",
    "- **Accuracy** provides an overall view of correctness but may not be suitable for imbalanced datasets.\n",
    "- **Precision** focuses on minimizing false positives.\n",
    "- **Recall** focuses on minimizing false negatives.\n",
    "- **F1-Score** balances precision and recall, making it useful for imbalanced datasets.\n",
    "- **ROC Curve and AUC-ROC** help assess a model's ability to distinguish between classes and set the appropriate classification threshold.\n",
    "\n",
    "In practice, the choice of evaluation metrics depends on the specific problem and the relative costs of false positives and false negatives.\n",
    "\n",
    "## 7. Probability Estimation\n",
    "\n",
    "One of the advantages of logistic regression is its ability to provide probability estimates. Instead of just classifying instances into one of two classes, you can obtain the probability that an instance belongs to a particular class. This is useful in applications like risk assessment or fraud detection.\n",
    "\n",
    "## 8. Code Example in Python\n",
    "\n",
    "Let's illustrate logistic regression with a simple Python example using the scikit-learn library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27aa6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, auc, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset for binary classification\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b17a04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce75925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1 Score: 1.00\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        32\n",
      "           1       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e25ccf",
   "metadata": {},
   "source": [
    "This code demonstrates how to train a logistic regression model for binary classification using the Iris dataset. It loads the dataset, splits it into training and testing sets, trains the model, makes predictions, and evaluates its performance.\n",
    "\n",
    "That concludes our tutorial on Logistic Regression for binary classification and probability estimation. Logistic Regression is a fundamental tool in machine learning, and understanding it is crucial for a wide range of data science applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b92064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred) \n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90c9d8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.22912737035179 1\n",
      "94.2033531701884 0\n",
      "99.79057878816691 2\n",
      "77.32337848068198 1\n",
      "75.12981894330335 1\n",
      "95.00924472170655 0\n",
      "90.41396410357152 1\n",
      "85.23276524503447 2\n",
      "78.06092724358322 1\n",
      "94.08071690241997 1\n",
      "77.42039003546799 2\n",
      "96.27361260062257 0\n",
      "96.92055048526598 0\n",
      "95.60592918838925 0\n",
      "97.65446304440218 0\n",
      "66.33333264206968 1\n",
      "97.32611376430009 2\n",
      "94.51508303347799 1\n",
      "82.00886912089929 1\n",
      "95.8520559720802 2\n",
      "95.87439028548367 0\n",
      "61.18890507434447 2\n",
      "95.63031849015965 0\n",
      "94.78641977790096 2\n",
      "97.96278144998497 2\n",
      "89.4350821883366 2\n",
      "93.0132417569576 2\n",
      "98.05000053096622 2\n",
      "96.09968177119924 0\n",
      "94.9518212823561 0\n",
      "99.32041667476321 0\n",
      "98.07112588080186 0\n",
      "89.00237476264081 1\n",
      "96.46836509859983 0\n",
      "98.26091560020343 0\n",
      "71.42442864030222 2\n",
      "81.21564984613232 1\n",
      "96.41675194303856 0\n",
      "97.81195954928307 0\n",
      "98.30825516568645 0\n",
      "80.55332306698226 2\n",
      "73.69852814763581 1\n",
      "75.1701803957671 1\n",
      "98.17703822243685 0\n",
      "96.66547652579736 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model.predict_proba(X_test))):\n",
    "    print(max(model.predict_proba(X_test)[i])*100, y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ff69c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2900.19\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0656022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared Score: 0.45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate the R-squared score\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(f\"R-squared Score: {r_squared:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
